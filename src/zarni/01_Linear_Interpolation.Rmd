---
title: "01_Linear_Interpolation"
author: "Zarni Htet (zh938@nyu.edu)"
date: March 15, 2018
output: github_document
---

###Linear Interpolation

This Markdown is filling the missing data for BMI and media exposure at asynchronous time points using linear interpolation. The project is supervised by Professor Marc Scott and Professor Daphna Harel. The data is from the Belle Lab at the Bellevue Hospital. More details of the project scope is in the repository under lit folder.

#### R Libraries

This code block has all the needed R libraries

```{r}
#For the dta raw files
library(foreign)
#For importing different types of data set without specification
library(rio)
#For processing long form data
library(dplyr)
#GTools library for ordering numeric variables
library(gtools)
#For filling NA values
library(tidyr)
```

#### Uploading Raw data

In this code chunk, we are uploading raw .dta data and converting to it a csv. This will then be saved to a processing data folder to protect the integrity of the raw data.

```{r}
#The BMI data extract
bmi <- read.dta("../../data/raw/MASextract1.dta")
#The Media data extract
media <- read.dta("../../data/raw/MASextract2.dta")
#Writing the BMI data to processing file
write.csv(bmi, "../../data/processing/bmi.csv")
#Writing the media data to processing file
write.csv(media, "../../data/processing/media.csv")
```

#### Loading the Data back from Processing Folder

This code chunk is loading the working version of the data extra to be used throughout the document.

```{r}
#processing bmi data
p_bmi <- import("../../data/processing/bmi.csv")
p_media <- import("../../data/processing/media.csv")
```


```{r}
p_me

```



#### Data Exploration

This code chunks examine the two data sets. In particular, the focus here is on the key variables and the time intervals they are recorded. At the end of each code block for each data set, there is a short summary of what the data consists of.

#####The BMI data set overview

```{r}
head(p_bmi)
tail(p_bmi)
dim(p_bmi) #10326, 4
#check the number of unique subjects
length(unique(p_bmi$ID_)) #667 
length(unique(p_bmi$AgeMos)) #1951
print(sum(is.na(p_bmi$ID_))) #0 no values are missing here
print(sum(is.na(p_bmi$AgeMos))) #0 no values are missing here
```

Each subject has different time points. For subject 1, months may be 0, 0.5, 1.0 while subject 2 has months in 0, 0.7, 1.2 etc.


This is to explore the number of time intervals each subject has. 

```{r}
#This uses dplyr to group by each subject and count their instances. This effectively counts the number of time points each of them have.
bmi_timed <- p_bmi %>% 
  group_by(ID_) %>%
  summarize(n = n()) 
print(bmi_timed)
```

We will do a quick barplot to see the distribution of time points for each subject has

```{r}
#Using the table function and barplot to draw the distribution of time. 
barplot(table(bmi$ID_), main = "Time Count Distribution \n for Each Subject for BMI")
```

Check the Minimum/Maximum time intervals. This is to see if we have to explore edge cases later down the road for Last Value Carried forward at the end for each subject

```{r}
min(bmi_timed$n) #1
max(bmi_timed$n) #39
```

At least 1 subject has only 1 time interval for BMI. 
These **singletons** will be applied **LOCF** or **LOCB**.

##### Media exposure data set overview

```{r}
head(p_media)
tail(p_media)
dim(p_media) #1639, 5
#check the number of unique subjects
length(unique(p_media$ID_)) #542 
length(unique(p_media$AgeMos)) #745
print(sum(is.na(p_media$ID_))) #0
print(sum(is.na(p_media$AgeMos))) #0
```

This is to explore the number of time intervals each subject has. 

```{r}
#This uses dplyr to group by each subject and count their instances. This effectively counts the number of time points each of them have.
media_timed <- p_media %>% 
  group_by(ID_) %>%
  summarize(n = n()) 
print(media_timed)
```

Like the BMI from before, each subject has different count of time as well as time intervals where the data is collected.

```{r}
#Using the table function and barplot to draw the distribution of time. 
barplot(table(bmi$ID_), main = "Time Count Distribution \n for Each Subject for Media Exposure")
```

#####Cleaning data

In this section, I will attempt to discover the subjects that only have 1 data point for the BMI data set or the Media exposure data set. **[EDIT]**These will have to be removed from the working data set as Linear interpolation cannot happen unless we have more than 1 data point. Those subjects will be noted down and saved in a separate file.**[EDIT]**

**[UPDATE_1]** The singletons will **NOT** be removed from the file. Instead, LOCF and LOCB will be applied. This will be done once, the interpolation function has been tested.

**[UPDATE_2]** The singletons *MUST* be removed from the file or be expanded. The approx function cannot handle a case where there is only 1 single point. Addendum markdown has been added to explore the Singleton case. The question is to which time points does the singleton value has to be applied to. It must have a corresponding subject ID match from media at least to get another valid time point.

**[UPDATE_3]**
*Based on Daphna's Feedback*

The singletons will be handled in 4 different ways.

* If there is only one timestamp for the ID for BOTH BMI and Media (that is they were both only collected once and at the same time point), then leave it as it is.

*For coding purposes in Linear Interpolation, this data set has to be taken out while the others are being interpolated and then, merged later*

[UPDATE] Results of such a case is none.

* If one variable was collected once, and the other was collected serveral times, the LOCF/LOCB to fill in the blanks

[Method] To be used as a different function

* If both variables were collected once, but at differnent time points, then merge those two values (equivalent to LOCF/LOCB)

[Method] To be used as a different function

* If one variable is collected, and the other is never collected - we have to drop them from the dataset I think.

*This 4th case is taken care of by the Inner Join to be implemented later*

###### BMI cleaning

The objective of this section is to temporarily remove the data set portion of bullet point 1 above where there is only 1 time each data set and the data set time stamps match.  

Getting the IDs for the Singleton cases in both data sets

```{r}
#1) Get the Singleton IDs of both Data Sets

#An assumption has been that for each row, there is no missing corresponding time value or bmi value. This assumption holds because of the missingness checks above.

### GET the BMI data set singletons
bmi_exclude <- bmi_timed[bmi_timed$n==1,]
### GET the MEDIA data set singletons
media_exclude <- media_timed[media_timed$n==1,]
### GET ALL THE SINGLETON IDS in one data set
all_singletons <- rbind(bmi_exclude, media_exclude)
```

####Merging the two data sets

#####Join the two tables to fill in missing Xs and missing Ys for Each Subject

There are 10 steps in this section. At the end, we come out with an entirely interpolated data set. An Appendix section goes through the concept of Linear Interpolation as well as testing the Approx function that has been used extensively in this section. Other tests of the custom function that is written has been removed. Should it need to be in the Appendix, we can easily provide.

*Step 1: Finding the Common ID*

```{r}
common_ID <- intersect(p_bmi$ID_, p_media$ID_) # intersection works like in Set theory
length(common_ID) #537 common subject IDs
```

*Step 2: Extracting BMI and Media Data set based on shared ID*

```{r}
#Matched BMI
m_bmi <- p_bmi[(p_bmi$ID_ %in% common_ID), ]
#Matched Media
m_media <- p_media [(p_media$ID_ %in% common_ID),]
```

*Step 3: Generating NAs for each of the data set*
This is referencing Professor Harel's code of spliting and merging data

```{r}
#BMI data table first!
#First removing column V1 if it exists
if("V1" %in% colnames(m_bmi)){
  m_bmi <- m_bmi[,2:ncol(m_bmi)]
}
#Adding NAs in the BMI table by expanding the column
m_bmi <- cbind(m_bmi[,c(1,2)], NA, m_bmi[,3])
colnames(m_bmi) <- c("ID_", "AgeMos", "NA", "zBMI")

#MEDIA data table second!
if("V1" %in% colnames(m_media)){
 m_media <- m_media[,2:ncol(m_media)] 
}
#Adding NAs in the MEDIA table by expanding the column
m_media <- cbind(m_media, NA)
#Also need to drop squrt time spent as it can be generated by the linear Time
m_media <- m_media[,-4]
```

*Step 4: Merging the two data sets together*

```{r}
#The Column Names have to match for the data set to match together
colnames(m_bmi) <- c("ID", "Months", "Media", "zBMI")
colnames(m_media) <- c("ID", "Months", "Media", "zBMI")
#Combined Data Set
c_data <- rbind(m_bmi, m_media)
#Ordering to observe missing gaps in each data set
c_data <- c_data[order(c_data[,1]),]
#Convert all the time variables into numeric for later sorting
c_data$Months <- as.numeric(as.character(c_data$Months))
```

*Step 4.5: Arrange the data set of each SubjectID by Time*

```{r}
#dplyr command that does the arrange by the Group ID then within the groups
#arrange it by Months
c_data_arr<- c_data %>% arrange(ID,Months)
View(c_data_arr)
```

*Step 5: Checking # of duplicated values for each time value within each subject*

```{r}
#This is to check if BMI and Media has been recorded at the same time.
#If so, we want to merge those two rows in the data set.
dup_count <- c_data_arr %>% group_by(ID, Months) %>% summarise(n=n())
#View a subset of all the duplicate rows
v_dup <- dup_count[dup_count$n >1,]
print(head(v_dup))
print(dim(v_dup))

```

*Step 6: Merging duplicated row into 1 unit*

For merging rows, we are essentially saying between two values in both rows, do not pick NA, pick the value. Below is a custom-built function that achieves that. Said function will be applied to the dplyr summarize_each (which is essentially, apply this function to each row of each column).

```{r}
#The ifelse commands literally says, if not all of the x vector is NA, pick the maximum after removing the NA. Otherwise, keep the NA.
my.max <- function(x) ifelse(!all(is.na(x)),max(x, na.rm = T), NA)
```

```{r}
#If BMI and Media are recorded at the same time month, there should not be two separate rows for it.
#Combined data that is arranged and merged.
c_data_arr_mer <- c_data_arr %>% group_by(ID,Months) %>% summarise_each(funs(my.max))
#View(c_data_arr_mer)
#This is merged and cleaned data with the NAs
write.csv(c_data_arr_mer, "../../data/final/final_na_data.csv")
View(c_data_arr_mer)
```


*Step 7: Removing the Singleton Cases from the Data Set to handle Case I- Case III separately*
```{r}
#Getting all the rows with the Singleton IDs
singleton_data <- c_data_arr_mer [(c_data_arr_mer$ID %in% all_singletons$ID_),]
#Getting all the rows with non Singleton IDs
non_singleton_data <- c_data_arr_mer[!(c_data_arr_mer$ID %in% all_singletons$ID_),]
```

*Step 8: Handling Singleton Data 3 Cases*

The Singleton data will be combined back once the Singular NAs has been replaced appropriate with LOCF & LOCB which can then supplied into our approxfun as described in details in the Appendix.


```{r}
###Case I: Locating rows where there is only 1 time stamp for BMI and Media

#Check if there are cases of row n = 1

singleton_1 <- singleton_data %>% group_by(ID) %>%
                                  summarise(n=n())
nrow(singleton_1[singleton_1$n ==1,]) #0
###Case I issues does not exist.
```

```{r}
### Case II: LOCF/LOBF for BMI Singleton Cases

### Below function takes care of LOCF, LOBF for Singleton Cases of 1 unit value
### across multiple time periods

fill_NA <- function(df){
  
  #Saving the data frame in a local variable
  x <- df
  #total length of data frame
  total <- nrow(df)
  #total length of NA values in media
  total_na_media <- sum(is.na(df$Media))
  #total length of NA values in bmi
  total_na_bmi <- sum(is.na(df$zBMI))
  
  #Replacing the Media NA values if there is only 1 singular non-NA
  if (total-total_na_media == 1){
    #Get the replace value which is from non-NA
    replace_value <- df$Media[which(!is.na(df$Media))]
    #Replace it to the rest of the vector
    df$Media[which(is.na(df$Media))] <- replace_value
  }
  #Replacing the zBMI NA values if there is only 1 singular non-NA
  if (total-total_na_bmi == 1){
    #Get the replace value which is from non-NA
    replace_value <- df$zBMI[which(!is.na(df$zBMI))]
    #Replace it to the rest of the vector
    df$zBMI[which(is.na(df$zBMI))] <- replace_value
  }
  return(df)
}

#Split by each groupID
singleton_data_split <- split(singleton_data, singleton_data[,1])
#Apply NA fixes to each of the data splits
singleton_NA_filled <- lapply(singleton_data_split, fill_NA)
#Collapse the Split Data into a single data frame
singleton_NA_filled <- bind_rows(singleton_NA_filled)
write.csv(singleton_NA_filled, "../../data/Intermediate/singleton_NA_filled.csv")
```

*Step 9: Recombine the Singleton and Non-Singleton Data Sets*
combined_data <- rbind(non_singleton_data, singleton_NA_filled)


*Step 10: Split the combined data set by the Subject ID *

Here we are spliting the data set by subject ID so that we can apply the
interpolation function to each of the Subject ID

```{r}
#Split the data by subject ID
combined_data_split <- split(combined_data, combined_data[,1])

```

*Step 11: Build Custom Function to Handle Interpolation*

Main Custom function that interpolates our data frame

```{r}
#The function will take in a data frame as well as an input vector that specifies which column indexes of the data frame are of interest for the interpolation. The reason we have the input vector to give us a flexible to use a single function which can deal with a large data frame whose multiple columns may need interpolation.

#df refers to the data frame of interest
#par is a vector that specifies the TWO indexes: 1 being time in this case and the other being the missing column index

mdz_interpolate <- function(df, par){
  #Saving the data frame in a local variable
  x <- df
  #Pulling out the index for X vector (In our case Time)
  x_1 <- par[1]
  #Pulling out the index for Y Vector 1 (In our case BMI)
  y_1 <- par[2]
  #Pulling out the index for Y Vector 2 (In our case Media)
  y_2 <- par[3]
  #Pulling out x and y vectors for the interpolation
  #They are in data frame format.You have to unlist and make it a numeric vector.
  xx <- as.numeric(unlist(x[,x_1])) # X vector
  yy_1 <- as.numeric(unlist(x[,y_1])) # Y vector 1
  yy_2 <- as.numeric(unlist(x[,y_2])) # Y vector 2
  #Specifying indexes where we had to fill with the missing NA for y Vector 1
  xout_1 <- which(is.na(yy_1))
  #specifying indexes where we had to fill with the missing NA for y Vector 2
  xout_2 <- which(is.na(yy_2))
  #Specifying the minimum and maximum values for Last Value Carried Backward/Forward
  #Get the non-missing indexes first
  y_nmis_1 <- which(!is.na(yy_1))
  y_nmis_2 <- which(!is.na(yy_2))
  #Get the value from the furthest left index of Y (LOCB)
  y_min_1 <- yy_1[min(y_nmis_1)]
  y_min_2 <- yy_2[min(y_nmis_2)]
  #Get the value from the furthest right index of Y (LOCF)
  y_max_1 <- yy_1[max(y_nmis_1)]
  y_max_2 <- yy_2[max(y_nmis_2)]
  #Apply this to the interpolation function (Explanations of the function are in Appendix section)
  #The interpolation for the first vector
  out_1 <- approx(xx, yy_1, xout = xout_1,  method = "linear", yleft = y_min_1, yright = y_max_1, rule = 2)
  #The interpolation for the second vector
  out_2 <- approx(xx, yy_2, xout = xout_2, method = "linear", yleft = y_min_2, yright = y_max_2, rule = 2)
  #The missing values replaced data frame 1 of replaced Vector 1
  outframe_1 <- d_replace(x, out_1, 3)
  #The missing values replaced data frame 2 of replaced Vector 2
  outframe_2 <- d_replace(x, out_2, 4)
  outframe_1[,4] <- outframe_2[,4]
  return(outframe_1)
}
```

Helper Function that puts missing values back into the data frame.

```{r}
#The function takes in an actual data frame (df), an Robject of the interpolation function which contains the index values that has been replaced under vector x and the values that has been imputed under vector y. Then, we specify the column to which those values are replaced with rcol
d_replace <- function(df, robj, rcol){
  
  #saving the local data frame
  df <- df
  #saving the local robject
  robj <- robj
  #specifying the rows and the columns to replace the R values by
  df[robj$x,rcol] <- robj$y 
  return(df)
}
```

*Step 12: Applies the Custom Interpolation Function to the Split data set*

```{r}
#The split data is put in and then, the time column: 2, the BMI column: 3 and the media column 4 are applied the interpolation function
c_data_interp <- lapply(combined_data_split, mdz_interpolate, par=c(2,3,4))
```

*Step 13: Collapse the Split Data Into 1 Data Frame*

```{r}
#Use dplyr bind_rows to recompose the split data together
#http://dplyr.tidyverse.org/reference/bind.html
c_data_interp_bind <- bind_rows(c_data_interp)
write.csv(c_data_interp_bind, "../../data/final/final_interp_data.csv")
```


### APPENDIX:

#### Section I: Theorectical Explanation

#####Base Linear Interpolation Function

The linear interpolation equation to be used in the base function is below. The $y_{0}$ and $y_{1}$ would be either BMI or Media exposure variable. The $x_{0}$ and $x_{1}$ would be the time variable.

The $y$ variable is the missing value we are looking for at time $x$. For BMI variable, the $x$ corresponds to time from Media exposure that is missing between the $x_{0}$ and the $x_{1}$ intervals. The converse can be said of the Media Exposure variable to BMI as well. 

Source: Linear Interpolation, Wikipedia
$$
y = y_{0} + (x - x_{0}) \frac{y_{1}- y_{0}}{x_{1} - x_{0}}
$$


This section deals with testing out functions and other stuffs

Use ApproxFun:
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/approxfun.html

#### Section II: Testing the Approx Function

##### START: Testing Out Linear Interpolation approx/approxfun

Both approx and approxfun looks fairly similar.
There are a couple of **key parameters** to consider
* x,y => input vectors
* xout => we specify which indexes we want to interpolate    values for
* yleft => this is specifying the last value to be carried to the left or backward if x values are less than min(x) 

* yright => this is specifying the last value to be carried to the right or forward if x values are more than max(x)

* rule => Two options. 1 is to get NA for yleft, yright case. 2 is to output yleft, yright cases

######Test Case 1

This is a simple case of some missing Ys with X values.
A manual calculation is done below to verify the answer.



This helper function puts back the output to the actual data frame.

Simulated data 1
```{r}
x_1 <- c(1,2,3,4,5,6)
y_1 <- c(3,NA,5,NA,NA,10)
xout_1 <- which(is.na(y_1)) #which returns the indexes where y_1 vector has NA values
```

Specifying y_left and y_right

This code chunk will tackle the case of last carried left/backward and last carried right/forward. The goal is to find the furthest left y index that is not NA and save the value. The same goes for the furthest right.

```{r}
y_nmis_1 <- which(!is.na(y_1)) #indexes of non-missing y values
y_min_1 <- y_1[min(y_nmis_1)] #get the value from the furtherest left index of y 
y_max_1 <- y_1[max(y_nmis_1)] #get the value from the furthest right index of y
```

Applying the function

This code chunk applies the function

```{r}
out_1 <- approx(x_1, y_1, xout = xout_1,  method = "linear", yleft = y_min_1, yright = y_max_1, rule = 2)
```

Interpolated results
```{r}
print(out_1$y)
```

Manual calculation to confirm it.

Notetoself: In the future, helper functions should be in a separate source file. Seek permission from MS/DH.

Base interpolation helper function

```{r}
#Note: Come back and write more comments later.

#The function takes in two pairs of point and the point you want to interpolate
lin_interpol <- function(y0,y1,x0,x1,x){
  y <- y0 + (x-x0) * ((y1-y0)/(x1-x0))
  return(y)
}
```

Manually outputting the three NA values from above

```{r}
res_1_1 <- lin_interpol(3,5,1,3,2)
print(res_1_1)
res_2_1 <- lin_interpol(5,10,3,6,4)
print(res_2_1)
res_3_1 <- lin_interpol(5,10,3,6,5)
print(res_3_1)
```

All the results matches up. We only have a case of Last Value Carried forward and backward to test

######Test Case 2

Simulated data 2

We are testing the case of last value carried forward with 1 value missing on the left and 2 values missing on the right
```{r}
x_2 <- c(1,2,3,4,5,6)
y_2 <- c(NA,3,5,10,NA,NA)
xout_2 <- which(is.na(y_2)) #which returns the indexes where y_1 vector has NA values
```

Same as above (Comments to merge or fill in later)

```{r}
y_nmis_2 <- which(!is.na(y_2)) #indexes of non-missing y values
y_min_2 <- y_2[min(y_nmis_2)] #get the value from the furtherest left index of y 
y_max_2 <- y_2[max(y_nmis_2)] #get the value from the furthest right index of y
```

This code chunk applies the function

```{r}
out_2 <- approx(x_2, y_2, xout = xout_2,  method = "linear", yleft = y_min_2, yright = y_max_2, rule = 2)
```

Interpolated results
```{r}
print(out_2$y)
```
Perfect. Left value carried forward and right value carried forward works like a charm.

##### END: Testing Out Linear Interpolation approx/approxfun











##### To Be Archived as R function is working as it should

Filling using Base Function above Function

```{r}
#Parameters, X and Y values of each subject with missing NAs for the Y values

#Within function
## skip the first time point
## from the second time point and onwards
### if a missing NA is encountered for Y, go to the non-missing X and Y pair and the next one before. 
### Calculate the time points in between.

#This could be much easily done if I use indexes of missing and non-missing.
##Have an index vector with the two X and Ys.
## Missing indexes can be two types
### It could be an index of count 1 and multiple
### For either case, pick the x0 and y0 and fill it up
```

Applying for Last Value Carried Forward Function

```{r}
#Apply Last Value Carried Forward/Backward for the values
```

Applying to all the subjects function
```{r}

```

Execution of the Functions

```{r}
#All BMI subjects
```

```{r}
#All Media Exposure subjects
```


























