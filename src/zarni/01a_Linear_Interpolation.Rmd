---
title: "Linear Interpolation"
author: "Zarni Htet "
email: "zh938@nyu.edu"
affiliation: "New York University"
date: March 15, 2018
output: github_document
---

### Introduction

The goal of this code file is to generate an imputed data set of **BMI** and **Media Exposure** for young children from low income families. The existing raw data set has been linearly interpolated to fill in the missing values for **BMI** and **Media Exposure** at missing time points. A sample of the raw data set along with the existing issue and one proposed method for resolution is demonstrated in the objective section below.

### Datasets Involved

Refer to the Belle Lab Power Point to add this portion


### Admnistration

The project is supervised by Professor Marc Scott and Professor Daphna Harel. The data is from the Belle Lab at the Bellevue Hospital. Additional background on the project is in the *README* at the root directory of the Github repository associated with the project. 

#### R Libraries

This block has all the *required* libraries for this code file.

```{r, R.options=FALSE, warning=FALSE, message=FALSE}
#For the dta raw files
library(foreign)
#For importing different types of data set without specification
library(rio)
#For processing long form data
library(dplyr)
#GTools library for ordering numeric variables
library(gtools)
#For filling NA values
library(tidyr)
#Loading Rmarkdown library for rendering
library(rmarkdown)
#knitr library for rendering
library(knitr)
#for missing data
library(mi)
#Sourcing the code file
source("../zarni/01b_Function_LInterpolation.R")
```

#### I: Uploading Raw data

In this section, the raw data is processed to a .csv format. The data files will then be saved to a processing data folder to protect the integrity of the raw data.

```{r, eval = FALSE, warning=FALSE}
#The BMI data extract
bmi <- read.dta("../../data/raw/MASextract1.dta")
#The Media data extract
media <- read.dta("../../data/raw/MASextract2.dta")
#Writing the BMI data to processing file
write.csv(bmi, "../../data/processing/bmi.csv")
#Writing the media data to processing file
write.csv(media, "../../data/processing/media.csv")
```

This code chunk is reloading and doing minor cleaning to the working version of the data to be used throughout the rest of the code file.

**Note to self**
Insert a custom function to remove the V1 from the data set.

```{r, echo=FALSE}
#processing bmi data
p_bmi <- import("../../data/processing/bmi.csv")
p_media <- import("../../data/processing/media.csv")
```


### Objective

The overarching goal of the project is to assess whether infant media exposure is associated with weight/bmi trajectories during their infant to early childhood periods. In order to examine that association, media exposure data and weight/bmi data need to be synchronized (exactly how the response will be modeled as a function of the predictors is to be determined, but a first step is examining aligned data).  As can be seen below in our short data snippet, media exposure and weight/bmi are collected at **asynchronous** time points. Our goal is to impute these missing data at missing time points via **linear interpolation**. For more on **linear interpolation**, please see the *Appendix* section.

```{r, warning=FALSE, echo=FALSE}
#Joining the two data sets to capture the missing values in each data set by ID and time
p_bmi_media_snippet <- p_bmi %>% full_join(p_media, by = c("ID_" = "ID_", "AgeMos" = "AgeMos")) 
p_bmi_media_snippet <- p_bmi_media_snippet[,c("ID_", "AgeMos", "zBMI", "lnmediatimespent", "sqrtmediatimespent")]
p_bmi_media_snippet <- p_bmi_media_snippet %>% arrange (ID_, AgeMos)
print(p_bmi_media_snippet[c(7:9),])
```

#### II: Data Exploration

This section examines the two datasets. The focus here is to explore the distribution of BMI and Media Exposure as well as cases of missing data and distribution of time points for each data set.

##### The BMI data set overview

```{r,echo= FALSE}
#p_bmi[1:15,]
head(p_bmi,15)
#tail(p_bmi)
```

Each subject has different time points. For subject 1, months may be 0, 0.13 , 0.55 while subject 2 has months in 0, 1.5, 2.5 etc.

```{r, echo=FALSE, results= "hide"}
dim(p_bmi) #10326, 4
#check the number of unique subjects
length(unique(p_bmi$ID_)) #there are 667 unique subject IDs
length(unique(p_bmi$AgeMos)) #there are 1951 unique time stamps
sum(is.na(p_bmi$ID_)) #0 no ID values are missing here
sum(is.na(p_bmi$AgeMos)) #0 no values are missing here
```

###### Missing data exploration

As can be seen from the visuals below, the BMI dataset by itself before joining across subjectID and time with the Media Exposure dataset has no missing values.

```{r, message=FALSE}
mdf_bmi = missing_data.frame(p_bmi)
```

```{r}
image(mdf_bmi)
```

###### Distribution of BMI values

The BMI values are more or less normalized as can be seen below.

```{r}
plot(density(p_bmi$zBMI), main = "Distrbution of BMI Values")
```

###### Distribution of Number of Time Intervals by Subject Count for BMI

The number of time points counted over the number of subject ID within each count of time points can be seen below. The majority of the subjects appear to have between 11 and 23 time points.

```{r, echo=FALSE}
bmi_tt <- table(table(p_bmi$ID_)) 
barplot(bmi_tt, main = "Number of Time Points Distribution by Subject Count")
```

The count of time points per subject ID is also calculated. As shown below, at least one subject ID has only **1** timepoint and **39** timepoints respectively. This insight is useful in that the built-in linear interpolation cannot impute data with only one timepoint. Therefore, those cases will have to be handled separately.

```{r}
bmi_timed <- p_bmi %>% 
  group_by(ID_) %>%
  summarize(n = n()) 
print(max(bmi_timed$n))
print(min(bmi_timed$n))
```

##### Media exposure data set overview

```{r, echo=FALSE}
p_media <- p_media %>% arrange(ID_, AgeMos)
p_media[1:15,]
#head(p_media)
#tail(p_media)
```

Like with the BMI data set before each Media data set subject has different time points. For subject 1, months are 7.786, 15.24 while subject 2 has months such as 6.73, 24.14 etc.

```{r, echo = FALSE, results= "hide"}
dim(p_media) #1639, 5
#check the number of unique subjects
length(unique(p_media$ID_)) #542 
length(unique(p_media$AgeMos)) #745
print(sum(is.na(p_media$ID_))) #0
print(sum(is.na(p_media$AgeMos))) #0
```

###### Missing data exploration

As can be seen from the visuals below, the Media dataset by itself before joining across subjectID and time with the BMI dataset has no missing values.

```{r, message=FALSE}
mdf_media = missing_data.frame(p_media)
```

```{r}
image(mdf_media)
```

###### Distribution of the Media Variable

The Media exposure data set has two measures of media time spent for infants. One is log transformed and the other is square root transformed. A plot of both transformations is compared below.

```{r}
par(mfrow =c(1,2))
plot(density(p_media$sqrtmediatimespent), main = "Sqrt Media Time Spent")
plot(density(p_media$lnmediatimespent), main = "Log Media Time Spent")
```

The square root transformation looks more **normal** than log transformation. Subsequently, it is utilized in the rest of the code file.

###### Distribution of Number of Time Intervals by Subject Count for Media Exposure

```{r}
#Using the table function and barplot to draw the distribution of time. 
media_tt = table(table(p_media$ID_))
barplot(media_tt, main = "Number of Time Points Distribution by Subject Count")
```

The number of time points counted over the number of subject ID within each count of time points can be seen below. Unlike BMI data set, all the Media data set subjects have time points between **1** and **5** inclusive. It indicates that more time points are missing in Media data set compared to the BMI data set. 

```{r, echo = FALSE, results="hide"}
#This uses dplyr to group by each subject and count their instances. This effectively counts the number of time points each of them have.
media_timed <- p_media %>% 
  group_by(ID_) %>%
  summarize(n = n()) 
print(min(media_timed$n))
print(max(media_timed$n))
```

#### III: Interpolation Scenarios

This section lays out the different interpolation scenarios as the two data sets are joined to capture missing values in each data set. The goal as stated before is to impute these values using **linear interpolation**.

###### Scenario I:

In this case, there are missing BMI and Media Exposure values at varying time points. The built-in linear interpolation function (as described in the *Appendix*) can be applied to each value column of BMI and Media Exposure. The function will work smoothly for cases like these as there is at least **two** values for the respective missing columns. If there are less than **two** non-missing values in BMI or Media Exposure column, the built-in function will fail. Such cases are handled with a custom function as demonstrated later. 

```{r, echo = FALSE}
print(p_bmi_media_snippet[c(6:9),])
```

###### Scenario II:

In this case, one column value (zBMI) is completely filled across all the time points. The Media Exposure value on the other hand has only **one** value. In that case, we cannot utilize the built-in linear interpolation function. A custom function has been created to execute last value carried forward and backward on these cases. This type of scenario can happen similarly to zBMI where it only has **one** value while the Media Exposure column is filled. Additionally, it could be that one of the columns does not necessarily have to be filled completely. Rather that it has at least **two** values or more. Those cases would fall under this scenario as well.

```{r, echo=FALSE}
print(p_bmi_media_snippet[p_bmi_media_snippet$ID_ == 3,])
```

###### Scenario III:

In this case, the subject ID only exists in one data set and does not exist in the other. The confirmation to that case is that all the Media Exposure values are NAs as shown below. As shown before, the Media data has no missing values by itself. Thus, this is a case of subject ID mismatch. There is no point in constructing a study where the subject altogether does not exist in one data set or the other. These cases will be taken care of through only matching common ids between the two data sets.

```{r, echo= FALSE}
#Find one ID where BMI and Media exposure does not match
id <- p_bmi[!p_bmi$ID_ %in% p_media$ID_,]$ID_[1]
print(p_bmi_media_snippet[p_bmi_media_snippet$ID_ == id,])
```

###### Scenario IV:

Scenario IV is where the subject ID from both BMI and Media data sets match. Additionally, they only have 1 time point each and that time point is exactly equal as well. In that case, there is **no** other time point to linearly interpolate. As a result such data rows must be temporarily removed before applying our linear interpolation functions. Fortunately, as can be seen below, there is only one subject ID of one time instance in both data sets that where the subject ID matches. However, the exact time instance does not match. Therefore, this scenario is avoided.

```{r, echo=FALSE}
#GET the BMI data set where there is only a single time point instance
bmi_exclude <- bmi_timed[bmi_timed$n==1,]
#GET the MEDIA data set where there is only a single time point instance
media_exclude <- media_timed[media_timed$n==1,]
#Gather all the Singleton Values in 1 data set for use later
all_singletons <- unique(rbind(bmi_exclude, media_exclude))
#Find the match of BMI and Media for single time points
matches <- bmi_exclude$ID_ %in% media_exclude$ID_
#As it matches in ID_626 let's check it out.
print(p_bmi_media_snippet[p_bmi_media_snippet$ID_ == 626,])
```

#### IV: Creating Linearly Interpolated Data Set

In this section, the end goal is to create a linearly interpolated data set while resolving scenario I to scenario III of interpolation from the above section. The section is subdivided into the following themes.

* Subsetting both data sets via **common** subject IDs betwen the two data sets. This action takes care of scenario III.
* Creating a combined from the two subsetted data sets above for interpolation.
* Applying custom last value carried forward and backward function to handle scenario II.
* Applying the built-in interpolation function to execute imputation for scenario I.

There themes are carried out in a total of **7** steps as demonstrated below.

###### Step 1: Finding the Common ID

Finding the subject IDs that match across both data sets

```{r, results= "hide"}
common_ID <- intersect(p_bmi$ID_, p_media$ID_) # intersection works like in Set theory
print(length(common_ID)) #537 common subject IDs
```

###### Step 2: Subsetting BMI and Media Data set based on shared ID

```{r}
#Matched BMI
m_bmi <- p_bmi[(p_bmi$ID_ %in% common_ID), ]
#Matched Media
m_media <- p_media[(p_media$ID_ %in% common_ID),]
```

###### Step 3: Combining both data sets for create missing time points in each data set

```{r}
#Merging both data sets by ID and Months  
c_data <- m_bmi %>% full_join(m_media, by =c("ID_" = "ID_", "AgeMos" = "AgeMos"))
#Extracting needed columns esp sqrtmediatimespent as it is more normal than the log transformation
c_data <- c_data[,c("ID_", "AgeMos","sqrtmediatimespent", "zBMI")]
#Renaming the variables
colnames(c_data) <- c("ID", "Months", "Media", "zBMI")

### Doing minor cleaning

#Converting Months into numeric for sorting later
c_data$Months <- as.numeric(as.character(c_data$Months))
#Converting the square root media value to square media for later linear interpolation
c_data$Media <- c_data$Media^2
#Rearrange the data by GroupID then within groups by Time
c_data_arr <- c_data %>% arrange(ID, Months) 
```

###### Missing Values Creation Confirmation

```{r, echo=FALSE, message=FALSE, results="hide"}
mdf_combined <- missing_data.frame(c_data_arr)
```

As shown in the picture below, Media exposure missing values and BMI missing values at various time points are created. As to be expected from the data exploration before, there are more Media exposure missing values than that of BMI.

```{r}
image(mdf_combined)
```


###### Step 4: Merging Duplicated Rows  

This section checks if BMI and Media data has been recorded at the same time for the same subject ID. If those rows exist, the goal is to merge those same rows by the **average** values of both BMI and Media Exposure duplicated columns. A sample of what these duplicated rows looks like is show below.

```{r}
#Group by gets the same ID and Month and summarise checks the count
dup_count <- c_data_arr %>% group_by(ID, Months) %>% summarise(n=n())
#Capture the duplicate cases
v_dup <- dup_count[dup_count$n >1,]
#Save an id of a duplicate case
id <- v_dup$ID[1]
#Spit out a duplicate case
print(c_data_arr[c_data_arr$ID == id,][3:6,])
```

###### Custom Function applied to merge the duplicated rows

```{r}
#Combined data that is arranged and merged.
c_data_arr_mer <- c_data_arr %>% group_by(ID,Months) %>% summarise_all(funs(my.rowmerge))
```

```{r}
#Saving the arranged and merged data set
write.csv(c_data_arr_mer, "../../data/final/final_na_data.csv")
```

###### Step 5: Handling data Scenario II from Section III   

As mentioned before, there are cases where there is only **1** single value of either BMI or Media Exposure data with the rest of the time points' values being missing. The built-in linear interpolation function cannot handle it.
Therefore, last value carried forward/backward will be carried out with our own custom functions.

###### Separate the data of those single time instance values and not

```{r}
#Getting all the rows with single value IDs
singleton_data <- c_data_arr_mer [(c_data_arr_mer$ID %in% all_singletons$ID_),]
#Getting all the rows with non single value IDs
non_singleton_data <- c_data_arr_mer[!(c_data_arr_mer$ID %in% all_singletons$ID_),]
```

###### Applying custom function for last value carried forward/backward

```{r}
#Using tapply to each of the IDs with the custom LOCF/LOCB function for BMI and Media
singleton_bmi <- tapply(singleton_data$zBMI, singleton_data$ID, function (x) {fill_NA(x)})
singleton_media <- tapply(singleton_data$Media, singleton_data$ID, function (x) {fill_NA(x)})
```

###### Attaching the LOCF/LOCB values back to data frame

```{r}
#Must combine the list of vectors by ID back into a single vector
singleton_bmi <- combine(singleton_bmi)
singleton_media <- combine(singleton_media)
#Put them back into the data frame
singleton_data$zBMI <- singleton_bmi
singleton_data$Media <- singleton_media
```

```{r}
#Saving the NA filled data for data integrity
write.csv(singleton_data, "../../data/Intermediate/singleton_NA_filled.csv")
```

```{r}
#Combinig the singleton and non-singleton data back
combined_data <- rbind(singleton_data, non_singleton_data)
combined_data <- combined_data %>% arrange(ID, Months)
```

###### Step 6: Applying the Linear Interpolation Function for Scenario I  

Notes to be written in paragraph form later.
* Refer to Appendix for approx breakdown
* Brief summary of the **important** parts of the function

##### Questions here!
```{r}
#Question:
#tapply would not split the rest of the arguments that I need to put in. What would be a better design?

#Split the one of the columns we are not interpolating by ID
media <- split(combined_data$Media, combined_data$ID)
#Things still needed
# 1) The indexes of where the replacement needs to be done
# 2) The min and max value at each ID for the vector we are interpolating for LOCB, LOCF

interpolate_bmi <- tapply(combined_data$zBMI, combined_data$ID, function(x){approx(x, y =)})
```







